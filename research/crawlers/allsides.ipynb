{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllSides article crawler\n",
    "\n",
    "Crawling all the articles from https://www.allsides.com/story/admin and exporting them in CSV format. For each article we extract:\n",
    "\n",
    "- Page nr.\n",
    "- Index on page\n",
    "- AllSides URL\n",
    "- Subject\n",
    "- Topic\n",
    "- Date\n",
    "\n",
    "_and for each 3-tuple of articles_\n",
    "\n",
    "- Title\n",
    "- Source\n",
    "- Label (Left/Right/Center/etc.)\n",
    "- Original article URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import re\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tpl = \"https://www.allsides.com/story/admin?page={}\"\n",
    "html_parser = \"html5lib\"\n",
    "bias_regex = re.compile(r\"Rating:\\s+([\\w\\s]+)\")\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    abs_url = urlparse.urljoin(url_tpl, url)\n",
    "    resp = requests.get(abs_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(resp.text, html_parser)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def parse_bundle(soup):\n",
    "    \"\"\"Retrieves 2/3 articles within the same subject page.\"\"\"\n",
    "    articles = []\n",
    "    for article_div in soup.find_all(\"div\", class_=\"quicktabs-views-group\"):\n",
    "        title_div = article_div.find(\"div\", class_=\"news-title\")\n",
    "        title = title_div.text\n",
    "        url = title_div.find(\"a\").get(\"href\")\n",
    "\n",
    "        src_area = article_div.find(\"div\", class_=\"source-area\")\n",
    "        src_div = src_area.find(\"div\", class_=\"news-source\")\n",
    "        if src_div:\n",
    "            source = src_div.text\n",
    "        else:\n",
    "            source = None\n",
    "            logging.warning(\"Article source not available.\")\n",
    "            \n",
    "        bias_div = src_area.find(\"img\", typeof=\"foaf:Image\")\n",
    "        if bias_div:\n",
    "            bias_text = bias_div.get(\"alt\")\n",
    "            label = bias_regex.search(bias_text).group(1)\n",
    "        else:\n",
    "            label = None\n",
    "            logging.warning(\"Article label not available.\")\n",
    "        \n",
    "        article = {\n",
    "            \"title\": title,\n",
    "            \"source\": source,\n",
    "            \"label\": label,\n",
    "            \"url\": url,\n",
    "        }\n",
    "        articles.append(article)\n",
    "        \n",
    "    logging.debug(\"Retrieved %d more articles.\", len(articles))\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_articles(soup):\n",
    "    \"\"\"Retrieves all the articles within a page.\"\"\"\n",
    "    all_articles = []\n",
    "    body = soup.find(\"tbody\")\n",
    "    if not body:\n",
    "        logging.warning(\"Reached empty page of results.\")\n",
    "        return all_articles\n",
    "    \n",
    "    for idx, row in enumerate(body.find_all(\"tr\")):\n",
    "        logging.debug(\"Getting row subject with index: %d.\", idx)\n",
    "        name_td = row.find(\"td\", class_=\"views-field-name\")\n",
    "        subject = name_td.text\n",
    "        bundle_url = name_td.find(\"a\").get(\"href\")\n",
    "        \n",
    "        topic = row.find(\"td\", class_=\"views-field-field-story-topic\").text\n",
    "        date = row.find(\"td\", class_=\"views-field-field-story-date\").text\n",
    "        \n",
    "        articles = parse_bundle(get_soup(bundle_url))\n",
    "        for article in articles:\n",
    "            article.update({\n",
    "                \"index_on_page\": idx,\n",
    "                \"allsides_url\": urlparse.urljoin(url_tpl, bundle_url),\n",
    "                \"subject\": subject,\n",
    "                \"topic\": topic,\n",
    "                \"date\": date,\n",
    "            })\n",
    "        all_articles.extend(articles)\n",
    "        \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate pages and process each row as a BS object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - root - 2019-04-01 20:08:33,734 - Crawling page 0...\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:33,755 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:33,963 - https://www.allsides.com:443 \"GET /story/admin?page=0 HTTP/1.1\" 200 19985\n",
      "DEBUG - root - 2019-04-01 20:08:34,138 - Getting row subject with index: 0.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:34,145 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:34,473 - https://www.allsides.com:443 \"GET /story/house-judiciary-committee-subpoena-mueller-report HTTP/1.1\" 200 19023\n",
      "DEBUG - root - 2019-04-01 20:08:34,622 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:34,623 - Getting row subject with index: 1.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:34,627 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:34,823 - https://www.allsides.com:443 \"GET /story/federal-judge-finds-trumps-order-oil-drilling-open-arctic-waters-unlawful HTTP/1.1\" 200 19267\n",
      "DEBUG - root - 2019-04-01 20:08:34,982 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:34,983 - Getting row subject with index: 2.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:34,988 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:35,270 - https://www.allsides.com:443 \"GET /story/trump-says-he-will-close-border-if-mexico-doesnt-stop-sending-migrants HTTP/1.1\" 200 19386\n",
      "DEBUG - root - 2019-04-01 20:08:35,425 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:35,426 - Getting row subject with index: 3.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:35,429 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:35,719 - https://www.allsides.com:443 \"GET /story/mays-brexit-deal-fails-third-time HTTP/1.1\" 200 19050\n",
      "DEBUG - root - 2019-04-01 20:08:35,863 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:35,863 - Getting row subject with index: 4.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:35,867 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:36,759 - https://www.allsides.com:443 \"GET /story/trump-looks-focus-healthcare-ahead-2020 HTTP/1.1\" 200 19242\n",
      "DEBUG - root - 2019-04-01 20:08:36,915 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:36,916 - Getting row subject with index: 5.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:36,919 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:37,699 - https://www.allsides.com:443 \"GET /story/trump-and-gop-lawmakers-call-adam-schiff-resign HTTP/1.1\" 200 19204\n",
      "DEBUG - root - 2019-04-01 20:08:37,836 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:37,837 - Getting row subject with index: 6.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:37,841 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:38,704 - https://www.allsides.com:443 \"GET /story/green-new-deal-dies-senate HTTP/1.1\" 200 19318\n",
      "DEBUG - root - 2019-04-01 20:08:38,875 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:38,880 - Getting row subject with index: 7.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:38,885 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:39,927 - https://www.allsides.com:443 \"GET /story/charges-against-jussie-smollett-dropped HTTP/1.1\" 200 19089\n",
      "DEBUG - root - 2019-04-01 20:08:40,076 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:40,077 - Getting row subject with index: 8.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:40,081 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:41,415 - https://www.allsides.com:443 \"GET /story/us-senate-vote-green-new-deal HTTP/1.1\" 200 19130\n",
      "DEBUG - root - 2019-04-01 20:08:41,590 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:41,590 - Getting row subject with index: 9.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:41,596 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:45,192 - https://www.allsides.com:443 \"GET /story/trump-admin-backs-court-ruling-would-invalidate-obamacare HTTP/1.1\" 200 19038\n",
      "DEBUG - root - 2019-04-01 20:08:45,344 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:45,345 - Getting row subject with index: 10.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:45,349 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:46,651 - https://www.allsides.com:443 \"GET /story/mueller-report-changes-landscape-trump HTTP/1.1\" 200 19138\n",
      "DEBUG - root - 2019-04-01 20:08:46,807 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:46,808 - Getting row subject with index: 11.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:46,813 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:47,533 - https://www.allsides.com:443 \"GET /story/mueller-report-finds-no-trump-russian-collusion HTTP/1.1\" 200 19400\n",
      "DEBUG - root - 2019-04-01 20:08:47,693 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:47,695 - Getting row subject with index: 12.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:47,702 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:48,434 - https://www.allsides.com:443 \"GET /story/fight-against-islamic-state-not-over-after-us-allied-syrian-force-declare-victory HTTP/1.1\" 200 19295\n",
      "DEBUG - root - 2019-04-01 20:08:48,596 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:48,596 - Getting row subject with index: 13.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:48,600 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:49,647 - https://www.allsides.com:443 \"GET /story/democrats-demands-mueller-report-be-released-public HTTP/1.1\" 200 19166\n",
      "DEBUG - root - 2019-04-01 20:08:49,822 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:49,823 - Getting row subject with index: 14.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:49,826 - Starting new HTTPS connection (1): www.allsides.com:443\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:50,949 - https://www.allsides.com:443 \"GET /story/michigan-stop-funding-adoption-agencies-bar-lgbt-couples HTTP/1.1\" 200 18974\n",
      "DEBUG - root - 2019-04-01 20:08:51,100 - Retrieved 3 more articles.\n",
      "DEBUG - root - 2019-04-01 20:08:51,101 - Getting row subject with index: 15.\n",
      "DEBUG - urllib3.connectionpool - 2019-04-01 20:08:51,106 - Starting new HTTPS connection (1): www.allsides.com:443\n"
     ]
    }
   ],
   "source": [
    "verbose = True  # make it True to see debugging messages\n",
    "level = logging.DEBUG if verbose else logging.INFO\n",
    "logging.root.handlers.clear()\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(name)s - %(asctime)s - %(message)s\",\n",
    "    level=level\n",
    ")\n",
    "\n",
    "dump_path = \"allsides.csv\"\n",
    "encoding = \"utf-8\"\n",
    "csv_header = [\n",
    "    \"page_nr\",\n",
    "    \"index_on_page\",\n",
    "    \"allsides_url\",\n",
    "    \"subject\",\n",
    "    \"topic\",\n",
    "    \"date\",\n",
    "    \"title\",\n",
    "    \"source\",\n",
    "    \"label\",\n",
    "    \"url\",\n",
    "]\n",
    "\n",
    "max_page = 100\n",
    "\n",
    "\n",
    "def postprocess_fields(article):\n",
    "    decode_str = lambda string: string.encode(\"windows-1252\").decode(\"utf-8\")\n",
    "    \n",
    "    _values = []\n",
    "    for key, value in article.items():\n",
    "        if isinstance(value, str):\n",
    "            value = value.strip()\n",
    "            bad_encoding = bool(list(filter(lambda char: ord(char) > 127, value)))\n",
    "            if bad_encoding:\n",
    "                logging.warning(\"Attempting to fix bad value %r.\", value)\n",
    "                for _ in range(2):\n",
    "                    try:\n",
    "                        value = decode_str(value)\n",
    "                    except UnicodeError:\n",
    "                        logging.error(\"Couldn't fix string value, leaving like it is.\")\n",
    "                        break\n",
    "        article[key] = value\n",
    "\n",
    "\n",
    "with open(dump_path, \"w\", newline=\"\", encoding=encoding) as csvfile:\n",
    "    article_writer = csv.DictWriter(csvfile, fieldnames=csv_header)\n",
    "    article_writer.writeheader()\n",
    "    \n",
    "    for page in range(max_page):\n",
    "        logging.info(\"Crawling page %d...\", page)\n",
    "        soup = get_soup(url_tpl.format(page))\n",
    "        articles = get_articles(soup)\n",
    "        if not articles:\n",
    "            break\n",
    "\n",
    "        logging.info(\"Dumping page %d with a total of %d articles.\", page, len(articles))\n",
    "        for article in articles:\n",
    "            article[\"page_nr\"] = page\n",
    "            postprocess_fields(article)\n",
    "            article_writer.writerow(article)\n",
    "            \n",
    "        csvfile.flush()\n",
    "            \n",
    "logging.info(\"Parsing AllSides just finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
