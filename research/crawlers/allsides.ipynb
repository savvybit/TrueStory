{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllSides article crawler\n",
    "\n",
    "Crawling all the articles from https://www.allsides.com/story/admin and exporting them in CSV format. For each article we extract:\n",
    "\n",
    "- Page nr.\n",
    "- Index on page\n",
    "- AllSides URL\n",
    "- Subject\n",
    "- Topic\n",
    "- Date\n",
    "\n",
    "_and for each 3-tuple of articles_\n",
    "\n",
    "- Title\n",
    "- Source\n",
    "- Label (Left/Right/Center/etc.)\n",
    "- Original article URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import re\n",
    "import urllib.parse as urlparse\n",
    "\n",
    "import bs4\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tpl = \"https://www.allsides.com/story/admin?page={}\"\n",
    "html_parser = \"html5lib\"\n",
    "bias_regex = re.compile(r\"Rating:\\s+([\\w\\s]+)\")\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    abs_url = urlparse.urljoin(url_tpl, url)\n",
    "    resp = requests.get(abs_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(resp.text, html_parser)\n",
    "    return soup\n",
    "\n",
    "\n",
    "def parse_bundle(soup):\n",
    "    \"\"\"Retrieves 2/3 articles within the same subject page.\"\"\"\n",
    "    articles = []\n",
    "    for article_div in soup.find_all(\"div\", class_=\"quicktabs-views-group\"):\n",
    "        title_div = article_div.find(\"div\", class_=\"news-title\")\n",
    "        title = title_div.text\n",
    "        url = title_div.find(\"a\").get(\"href\")\n",
    "\n",
    "        src_area = article_div.find(\"div\", class_=\"source-area\")\n",
    "        src_div = src_area.find(\"div\", class_=\"news-source\")\n",
    "        if src_div:\n",
    "            source = src_div.text\n",
    "        else:\n",
    "            source = None\n",
    "            logging.warning(\"Article source not available.\")\n",
    "            \n",
    "        bias_div = src_area.find(\"img\", typeof=\"foaf:Image\")\n",
    "        if bias_div:\n",
    "            bias_text = bias_div.get(\"alt\")\n",
    "            label = bias_regex.search(bias_text).group(1)\n",
    "        else:\n",
    "            label = None\n",
    "            logging.warning(\"Article label not available.\")\n",
    "        \n",
    "        article = {\n",
    "            \"title\": title,\n",
    "            \"source\": source,\n",
    "            \"label\": label,\n",
    "            \"url\": url,\n",
    "        }\n",
    "        articles.append(article)\n",
    "        \n",
    "    logging.debug(\"Retrieved %d more articles.\", len(articles))\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_articles(soup):\n",
    "    \"\"\"Retrieves all the articles within a page.\"\"\"\n",
    "    all_articles = []\n",
    "    body = soup.find(\"tbody\")\n",
    "    if not body:\n",
    "        logging.warning(\"Reached empty page of results.\")\n",
    "        return all_articles\n",
    "    \n",
    "    for idx, row in enumerate(body.find_all(\"tr\")):\n",
    "        logging.debug(\"Getting row subject with index: %d.\", idx)\n",
    "        name_td = row.find(\"td\", class_=\"views-field-name\")\n",
    "        subject = name_td.text\n",
    "        bundle_url = name_td.find(\"a\").get(\"href\")\n",
    "        \n",
    "        topic = row.find(\"td\", class_=\"views-field-field-story-topic\").text\n",
    "        date = row.find(\"td\", class_=\"views-field-field-story-date\").text\n",
    "        \n",
    "        articles = parse_bundle(get_soup(bundle_url))\n",
    "        for article in articles:\n",
    "            article.update({\n",
    "                \"index_on_page\": idx,\n",
    "                \"allsides_url\": urlparse.urljoin(url_tpl, bundle_url),\n",
    "                \"subject\": subject,\n",
    "                \"topic\": topic,\n",
    "                \"date\": date,\n",
    "            })\n",
    "        all_articles.extend(articles)\n",
    "        \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate pages and process each row as a BS object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-54abaa840845>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "verbose = True  # make it True to see debugging messages\n",
    "level = logging.DEBUG if verbose else logging.INFO\n",
    "logging.root.handlers.clear()\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s - %(name)s - %(asctime)s - %(message)s\",\n",
    "    level=level\n",
    ")\n",
    "\n",
    "dump_path = \"allsides.csv\"\n",
    "encoding = \"utf-8\"\n",
    "csv_header = [\n",
    "    \"page_nr\",\n",
    "    \"index_on_page\",\n",
    "    \"allsides_url\",\n",
    "    \"subject\",\n",
    "    \"topic\",\n",
    "    \"date\",\n",
    "    \"title\",\n",
    "    \"source\",\n",
    "    \"label\",\n",
    "    \"url\",\n",
    "]\n",
    "\n",
    "max_page = 100\n",
    "\n",
    "\n",
    "def postprocess_fields(article):\n",
    "    decode_str = lambda string: string.encode(\"windows-1252\").decode(\"utf-8\")\n",
    "    \n",
    "    _values = []\n",
    "    for key, value in article.items():\n",
    "        if isinstance(value, str):\n",
    "            value = value.strip()\n",
    "            bad_encoding = bool(list(filter(lambda char: ord(char) > 127, value)))\n",
    "            if bad_encoding:\n",
    "                logging.warning(\"Attempting to fix bad value %r.\", value)\n",
    "                for _ in range(2):\n",
    "                    try:\n",
    "                        value = decode_str(value)\n",
    "                    except UnicodeError:\n",
    "                        logging.error(\"Couldn't fix string value, leaving like it is.\")\n",
    "                        break\n",
    "        article[key] = value\n",
    "\n",
    "\n",
    "with open(dump_path, \"w\", newline=\"\", encoding=encoding) as csvfile:\n",
    "    article_writer = csv.DictWriter(csvfile, fieldnames=csv_header)\n",
    "    article_writer.writeheader()\n",
    "    \n",
    "    for page in range(max_page):\n",
    "        logging.info(\"Crawling page %d...\", page)\n",
    "        soup = get_soup(url_tpl.format(page))\n",
    "        articles = get_articles(soup)\n",
    "        if not articles:\n",
    "            break\n",
    "\n",
    "        logging.info(\"Dumping page %d with a total of %d articles.\", page, len(articles))\n",
    "        for article in articles:\n",
    "            article[\"page_nr\"] = page\n",
    "            postprocess_fields(article)\n",
    "            article_writer.writerow(article)\n",
    "            \n",
    "        csvfile.flush()\n",
    "            \n",
    "logging.info(\"Parsing AllSides just finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
